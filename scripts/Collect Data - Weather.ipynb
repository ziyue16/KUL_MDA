{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# set output path\n",
    "# os.chdir(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import errno\n",
    "import time\n",
    "import hashlib\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from typing import Callable\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class Core:\n",
    "\n",
    "    \"\"\"\n",
    "    Base class that provides features which are used across the package\n",
    "    \"\"\"\n",
    "\n",
    "    # Base URL of the Meteostat bulk data interface\n",
    "    endpoint: str = 'https://bulk.meteostat.net/v2/'\n",
    "\n",
    "    # Location of the cache directory\n",
    "    cache_dir: str = os.path.expanduser(\n",
    "        '~') + os.sep + '.meteostat' + os.sep + 'cache'\n",
    "\n",
    "    # Maximum age of a cached file in seconds\n",
    "    max_age: int = 24 * 60 * 60\n",
    "\n",
    "    # Maximum number of threads used for downloading files\n",
    "    max_threads: int = 1\n",
    "\n",
    "    def _get_file_path(\n",
    "        self,\n",
    "        subdir: str,\n",
    "        path: str\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Get the local file path\n",
    "        \"\"\"\n",
    "\n",
    "        # Get file ID\n",
    "        file = hashlib.md5(path.encode('utf-8')).hexdigest()\n",
    "\n",
    "        # Return path\n",
    "        return self.cache_dir + os.sep + subdir + os.sep + file\n",
    "\n",
    "    def _file_in_cache(\n",
    "        self,\n",
    "        path: str\n",
    "    ) -> bool:\n",
    "        \"\"\"\n",
    "        Check if a file exists in the local cache\n",
    "        \"\"\"\n",
    "\n",
    "        # Get directory\n",
    "        directory = os.path.dirname(path)\n",
    "\n",
    "        # Make sure the cache directory exists\n",
    "        if not os.path.exists(directory):\n",
    "            try:\n",
    "                os.makedirs(directory)\n",
    "            except OSError as creation_error:\n",
    "                if creation_error.errno == errno.EEXIST:\n",
    "                    pass\n",
    "                else:\n",
    "                    raise Exception(\n",
    "                        'Cannot create cache directory') from creation_error\n",
    "\n",
    "        # Return the file path if it exists\n",
    "        if os.path.isfile(path) and time.time() - \\\n",
    "                os.path.getmtime(path) <= self.max_age:\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    @staticmethod\n",
    "    def _processing_handler(\n",
    "        datasets: list,\n",
    "        load: Callable[[dict], None],\n",
    "        max_threads: int\n",
    "    ) -> None:\n",
    "\n",
    "        # Single-thread processing\n",
    "        if max_threads < 2:\n",
    "\n",
    "            for dataset in datasets:\n",
    "                load(*dataset)\n",
    "\n",
    "        # Multi-thread processing\n",
    "        else:\n",
    "\n",
    "            pool = ThreadPool(max_threads)\n",
    "            pool.starmap(load, datasets)\n",
    "\n",
    "            # Wait for Pool to finish\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "\n",
    "    def _load_handler(\n",
    "        self,\n",
    "        path: str,\n",
    "        columns: list,\n",
    "        types: dict,\n",
    "        parse_dates: list,\n",
    "        coerce_dates: bool = False\n",
    "    ) -> pd.DataFrame:\n",
    "\n",
    "        try:\n",
    "\n",
    "            # Read CSV file from Meteostat endpoint\n",
    "            df = pd.read_csv(\n",
    "                self.endpoint + path,\n",
    "                compression='gzip',\n",
    "                names=columns,\n",
    "                dtype=types,\n",
    "                parse_dates=parse_dates)\n",
    "\n",
    "            # Force datetime conversion\n",
    "            if coerce_dates:\n",
    "                df.iloc[:, parse_dates] = df.iloc[:, parse_dates].apply(\n",
    "                    pd.to_datetime, errors='coerce')\n",
    "\n",
    "        except BaseException:\n",
    "\n",
    "            # Create empty DataFrane\n",
    "            df = pd.DataFrame(columns=[*types])\n",
    "\n",
    "        # Return DataFrame\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def _validate_series(\n",
    "        df: pd.DataFrame,\n",
    "        station: str\n",
    "    ) -> pd.DataFrame:\n",
    "\n",
    "        # Add missing column(s)\n",
    "        if 'time' not in df.columns:\n",
    "            df['time'] = None\n",
    "\n",
    "        # Add weather station ID\n",
    "        df['station'] = station\n",
    "\n",
    "        # Set index\n",
    "        df = df.set_index(['station', 'time'])\n",
    "\n",
    "        # Return DataFrame\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def _weighted_average(step: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Calculate weighted average from grouped data\n",
    "        \"\"\"\n",
    "\n",
    "        data = np.ma.masked_array(step, np.isnan(step))\n",
    "        data = np.ma.average(data, axis=0, weights=data[:, -1])\n",
    "        data = data.filled(np.NaN)\n",
    "\n",
    "        return pd.DataFrame(data=[data], columns=step.columns)\n",
    "\n",
    "    @classmethod\n",
    "    def clear_cache(\n",
    "        cls,\n",
    "        max_age: int = None\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Clear the cache\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "\n",
    "            if os.path.exists(cls.cache_dir + os.sep + cls.cache_subdir):\n",
    "\n",
    "                # Set max_age\n",
    "                if max_age is None:\n",
    "                    max_age = cls.max_age\n",
    "\n",
    "                # Get current time\n",
    "                now = time.time()\n",
    "\n",
    "                # Go through all files\n",
    "                for file in os.listdir(\n",
    "                        cls.cache_dir + os.sep + cls.cache_subdir):\n",
    "\n",
    "                    # Get full path\n",
    "                    path = os.path.join(\n",
    "                        cls.cache_dir + os.sep + cls.cache_subdir, file)\n",
    "\n",
    "                    # Check if file is older than max_age\n",
    "                    if now - \\\n",
    "                            os.path.getmtime(path) > max_age and os.path.isfile(path):\n",
    "                        # Delete file\n",
    "                        os.remove(path)\n",
    "\n",
    "        except BaseException as clear_error:\n",
    "            raise Exception('Cannot clear cache') from clear_error\n",
    "\n",
    "    @staticmethod\n",
    "    def _degree_mean(data: pd.Series):\n",
    "        \"\"\"\n",
    "        Return the mean of a list of degrees\n",
    "        \"\"\"\n",
    "\n",
    "        rads = np.deg2rad(data)\n",
    "        sums = np.arctan2(np.sum(np.sin(rads)), np.sum(np.cos(rads)))\n",
    "        return (np.rad2deg(sums) + 360) % 360\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from math import cos, sqrt, radians\n",
    "from copy import copy\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Union\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class Stations(Core):\n",
    "\n",
    "    \"\"\"\n",
    "    Select weather stations from the full list of stations\n",
    "    \"\"\"\n",
    "\n",
    "    # The cache subdirectory\n",
    "    cache_subdir: str = 'stations'\n",
    "\n",
    "    # The list of selected weather Stations\n",
    "    stations = None\n",
    "\n",
    "    # Raw data columns\n",
    "    _columns: list = [\n",
    "        'id',\n",
    "        'name',\n",
    "        'country',\n",
    "        'region',\n",
    "        'wmo',\n",
    "        'icao',\n",
    "        'latitude',\n",
    "        'longitude',\n",
    "        'elevation',\n",
    "        'timezone',\n",
    "        'hourly_start',\n",
    "        'hourly_end',\n",
    "        'daily_start',\n",
    "        'daily_end'\n",
    "    ]\n",
    "\n",
    "    # Processed data columns with types\n",
    "    _types: dict = {\n",
    "        'id': 'string',\n",
    "        'name': 'object',\n",
    "        'country': 'string',\n",
    "        'region': 'string',\n",
    "        'wmo': 'string',\n",
    "        'icao': 'string',\n",
    "        'latitude': 'float64',\n",
    "        'longitude': 'float64',\n",
    "        'elevation': 'float64',\n",
    "        'timezone': 'string'\n",
    "    }\n",
    "\n",
    "    # Columns for date parsing\n",
    "    _parse_dates: list = [10, 11, 12, 13]\n",
    "\n",
    "    def _load(self) -> None:\n",
    "        \"\"\"\n",
    "        Load file from Meteostat\n",
    "        \"\"\"\n",
    "\n",
    "        # File name\n",
    "        file = 'stations/lib.csv.gz'\n",
    "\n",
    "        # Get local file path\n",
    "        path = self._get_file_path(self.cache_subdir, file)\n",
    "\n",
    "        # Check if file in cache\n",
    "        if self.max_age > 0 and self._file_in_cache(path):\n",
    "\n",
    "            # Read cached data\n",
    "            df = pd.read_pickle(path)\n",
    "\n",
    "        else:\n",
    "\n",
    "            # Get data from Meteostat\n",
    "            df = self._load_handler(\n",
    "                file,\n",
    "                self._columns,\n",
    "                self._types,\n",
    "                self._parse_dates,\n",
    "                True)\n",
    "\n",
    "            # Add index\n",
    "            df = df.set_index('id')\n",
    "\n",
    "            # Save as Pickle\n",
    "            if self.max_age > 0:\n",
    "                df.to_pickle(path)\n",
    "\n",
    "        # Set data\n",
    "        self.stations = df\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "\n",
    "        # Get all weather stations\n",
    "        self._load()\n",
    "\n",
    "        # Clear cache\n",
    "        if self.max_age > 0:\n",
    "            self.clear_cache()\n",
    "\n",
    "    def id(\n",
    "        self,\n",
    "        organization: str,\n",
    "        code: str\n",
    "    ) -> 'Stations':\n",
    "        \"\"\"\n",
    "        Get weather station by identifier\n",
    "        \"\"\"\n",
    "\n",
    "        # Create temporal instance\n",
    "        temp = copy(self)\n",
    "\n",
    "        if isinstance(code, str):\n",
    "            code = [code]\n",
    "\n",
    "        if organization == 'meteostat':\n",
    "            temp.stations = temp.stations[temp.stations.index.isin(code)]\n",
    "        else:\n",
    "            temp.stations = temp.stations[temp.stations[organization].isin(\n",
    "                code)]\n",
    "\n",
    "        # Return self\n",
    "        return temp\n",
    "\n",
    "    def nearby(\n",
    "        self,\n",
    "        lat: float,\n",
    "        lon: float,\n",
    "        radius: int = None\n",
    "    ) -> 'Stations':\n",
    "        \"\"\"\n",
    "        Sort/filter weather stations by physical distance\n",
    "        \"\"\"\n",
    "\n",
    "        # Create temporal instance\n",
    "        temp = copy(self)\n",
    "\n",
    "        # Calculate distance between weather station and geo point\n",
    "        def distance(station, point) -> float:\n",
    "            # Earth radius in m\n",
    "            radius = 6371000\n",
    "\n",
    "            x = (radians(point[1]) - radians(station['longitude'])) * \\\n",
    "                cos(0.5 * (radians(point[0]) + radians(station['latitude'])))\n",
    "            y = (radians(point[0]) - radians(station['latitude']))\n",
    "\n",
    "            return radius * sqrt(x * x + y * y)\n",
    "\n",
    "        # Get distance for each stationsd\n",
    "        temp.stations['distance'] = temp.stations.apply(\n",
    "            lambda station: distance(station, [lat, lon]), axis=1)\n",
    "\n",
    "        # Filter by radius\n",
    "        if radius is not None:\n",
    "            temp.stations = temp.stations[temp.stations['distance'] <= radius]\n",
    "\n",
    "        # Sort stations by distance\n",
    "        temp.stations.columns.str.strip()\n",
    "        temp.stations = temp.stations.sort_values('distance')\n",
    "\n",
    "        # Return self\n",
    "        return temp\n",
    "\n",
    "    def region(\n",
    "        self,\n",
    "        country: str,\n",
    "        state: str = None\n",
    "    ) -> 'Stations':\n",
    "        \"\"\"\n",
    "        Filter weather stations by country/region code\n",
    "        \"\"\"\n",
    "\n",
    "        # Create temporal instance\n",
    "        temp = copy(self)\n",
    "\n",
    "        # Country code\n",
    "        temp.stations = temp.stations[temp.stations['country'] == country]\n",
    "\n",
    "        # State code\n",
    "        if state is not None:\n",
    "            temp.stations = temp.stations[temp.stations['region'] == state]\n",
    "\n",
    "        # Return self\n",
    "        return temp\n",
    "\n",
    "    def bounds(\n",
    "        self,\n",
    "        top_left: tuple,\n",
    "        bottom_right: tuple\n",
    "    ) -> 'Stations':\n",
    "        \"\"\"\n",
    "        Filter weather stations by geographical bounds\n",
    "        \"\"\"\n",
    "\n",
    "        # Create temporal instance\n",
    "        temp = copy(self)\n",
    "\n",
    "        # Return stations in boundaries\n",
    "        temp.stations = temp.stations[\n",
    "            (temp.stations['latitude'] <= top_left[0]) &\n",
    "            (temp.stations['latitude'] >= bottom_right[0]) &\n",
    "            (temp.stations['longitude'] <= bottom_right[1]) &\n",
    "            (temp.stations['longitude'] >= top_left[1])\n",
    "        ]\n",
    "\n",
    "        # Return self\n",
    "        return temp\n",
    "\n",
    "    def inventory(\n",
    "        self,\n",
    "        granularity: str,\n",
    "        required: Union[bool, datetime, tuple]\n",
    "    ) -> 'Stations':\n",
    "        \"\"\"\n",
    "        Filter weather stations by inventory data\n",
    "        \"\"\"\n",
    "\n",
    "        # Create temporal instance\n",
    "        temp = copy(self)\n",
    "\n",
    "        if required is True:\n",
    "            # Make sure data exists at all\n",
    "            temp.stations = temp.stations[\n",
    "                (pd.isna(temp.stations[granularity + '_start']) == False)\n",
    "            ]\n",
    "        elif isinstance(required, tuple):\n",
    "            # Make sure data exists across period\n",
    "            temp.stations = temp.stations[\n",
    "                (pd.isna(temp.stations[granularity + '_start']) == False) &\n",
    "                (temp.stations[granularity + '_start'] <= required[0]) &\n",
    "                (\n",
    "                    temp.stations[granularity + '_end'] +\n",
    "                    timedelta(seconds=temp.max_age)\n",
    "                    >= required[1]\n",
    "                )\n",
    "            ]\n",
    "        else:\n",
    "            # Make sure data exists on a certain day\n",
    "            temp.stations = temp.stations[\n",
    "                (pd.isna(temp.stations[granularity + '_start']) == False) &\n",
    "                (temp.stations[granularity + '_start'] <= required) &\n",
    "                (\n",
    "                    temp.stations[granularity + '_end'] +\n",
    "                    timedelta(seconds=temp.max_age)\n",
    "                    >= required\n",
    "                )\n",
    "            ]\n",
    "\n",
    "        return temp\n",
    "\n",
    "    def convert(\n",
    "        self,\n",
    "        units: dict\n",
    "    ) -> 'Stations':\n",
    "        \"\"\"\n",
    "        Convert columns to a different unit\n",
    "        \"\"\"\n",
    "\n",
    "        # Create temporal instance\n",
    "        temp = copy(self)\n",
    "\n",
    "        # Change data units\n",
    "        for parameter, unit in units.items():\n",
    "            if parameter in temp.stations.columns.values:\n",
    "                temp.stations[parameter] = temp.stations[parameter].apply(\n",
    "                    unit)\n",
    "\n",
    "        # Return class instance\n",
    "        return temp\n",
    "\n",
    "    def count(self) -> int:\n",
    "        \"\"\"\n",
    "        Return number of weather stations in current selection\n",
    "        \"\"\"\n",
    "\n",
    "        return len(self.stations.index)\n",
    "\n",
    "    def fetch(\n",
    "        self,\n",
    "        limit: int = None,\n",
    "        sample: bool = False\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Fetch all weather stations or a (sampled) subset\n",
    "        \"\"\"\n",
    "\n",
    "        # Copy DataFrame\n",
    "        temp = copy(self.stations)\n",
    "\n",
    "        # Return limited number of sampled entries\n",
    "        if sample and limit:\n",
    "            return temp.sample(limit)\n",
    "\n",
    "        # Return limited number of entries\n",
    "        if limit:\n",
    "            return temp.head(limit)\n",
    "\n",
    "        # Return all entries\n",
    "        return temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Point Class\n",
    "\n",
    "Meteorological data provided by Meteostat (https://dev.meteostat.net)\n",
    "under the terms of the Creative Commons Attribution-NonCommercial\n",
    "4.0 International Public License.\n",
    "\n",
    "The code is licensed under the MIT license.\n",
    "\"\"\"\n",
    "\n",
    "from datetime import datetime\n",
    "from meteostat import Stations\n",
    "\n",
    "\n",
    "class Point:\n",
    "\n",
    "    \"\"\"\n",
    "    Automatically select weather stations by geographic location\n",
    "    \"\"\"\n",
    "\n",
    "    # The interpolation method (weighted or nearest)\n",
    "    method: str = 'weighted'\n",
    "\n",
    "    # Maximum radius for nearby stations\n",
    "    radius: int = 35000\n",
    "\n",
    "    # Maximum difference in altitude\n",
    "    alt_range: int = 350\n",
    "\n",
    "    # Maximum number of stations\n",
    "    max_count: int = 4\n",
    "\n",
    "    # Adapt temperature data based on altitude\n",
    "    adapt_temp: bool = True\n",
    "\n",
    "    # Distance Weight\n",
    "    weight_dist: float = 0.6\n",
    "\n",
    "    # Altitude Weight\n",
    "    weight_alt: float = 0.4\n",
    "\n",
    "    # The latitude\n",
    "    lat: float = None\n",
    "\n",
    "    # The longitude\n",
    "    lon: float = None\n",
    "\n",
    "    # The altitude\n",
    "    alt: int = None\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        lat: float,\n",
    "        lon: float,\n",
    "        alt: int = None\n",
    "    ) -> None:\n",
    "\n",
    "        self.lat = lat\n",
    "        self.lon = lon\n",
    "        self.alt = alt\n",
    "\n",
    "        if alt is None:\n",
    "            self.adapt_temp = False\n",
    "\n",
    "    def get_stations(self, granularity: str, start: datetime, end: datetime):\n",
    "        \"\"\"\n",
    "        Get list of nearby weather stations\n",
    "        \"\"\"\n",
    "\n",
    "        # Get nearby weather stations\n",
    "        stations = Stations()\n",
    "        stations = stations.nearby(self.lat, self.lon, self.radius)\n",
    "\n",
    "        # Guess altitude if not set\n",
    "        if self.alt is None:\n",
    "            self.alt = stations.fetch().head(self.max_count)[\n",
    "                'elevation'].mean()\n",
    "\n",
    "        # Apply inventory filter\n",
    "        stations = stations.inventory(granularity, (start, end))\n",
    "\n",
    "        # Apply altitude filter\n",
    "        stations = stations.fetch()\n",
    "        stations = stations[abs(self.alt -\n",
    "                                stations['elevation']) <= self.alt_range]\n",
    "\n",
    "        # Calculate score values\n",
    "        stations['score'] = ((1 - (stations['distance'] / self.radius)) * self.weight_dist) + (\n",
    "            (1 - (abs(self.alt - stations['elevation']) / self.alt_range)) * self.weight_alt)\n",
    "\n",
    "        # Sort by score (descending)\n",
    "        stations = stations.sort_values('score', ascending=False)\n",
    "\n",
    "        return stations.head(self.max_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from copy import copy\n",
    "from datetime import datetime\n",
    "from typing import Union\n",
    "from numpy import NaN\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "class Daily(Core):\n",
    "\n",
    "    \"\"\"\n",
    "    Retrieve daily weather observations for one or multiple weather stations or\n",
    "    a single geographical point\n",
    "    \"\"\"\n",
    "\n",
    "    # The cache subdirectory\n",
    "    cache_subdir: str = 'daily'\n",
    "\n",
    "    # The list of weather Stations\n",
    "    stations = None\n",
    "\n",
    "    # The start date\n",
    "    start: datetime = None\n",
    "\n",
    "    # The end date\n",
    "    end: datetime = None\n",
    "\n",
    "    # Include model data?\n",
    "    model: bool = True\n",
    "\n",
    "    # The data frame\n",
    "    data = pd.DataFrame()\n",
    "\n",
    "    # Columns\n",
    "    _columns: list = [\n",
    "        'date',\n",
    "        'tavg',\n",
    "        'tmin',\n",
    "        'tmax',\n",
    "        'prcp',\n",
    "        'snow',\n",
    "        'wdir',\n",
    "        'wspd',\n",
    "        'wpgt',\n",
    "        'pres',\n",
    "        'tsun'\n",
    "    ]\n",
    "\n",
    "    # Data tapes\n",
    "    _types: dict = {\n",
    "        'tavg': 'float64',\n",
    "        'tmin': 'float64',\n",
    "        'tmax': 'float64',\n",
    "        'prcp': 'float64',\n",
    "        'snow': 'float64',\n",
    "        'wdir': 'float64',\n",
    "        'wspd': 'float64',\n",
    "        'wpgt': 'float64',\n",
    "        'pres': 'float64',\n",
    "        'tsun': 'float64'\n",
    "    }\n",
    "\n",
    "    # Columns for date parsing\n",
    "    _parse_dates: dict = {\n",
    "        'time': [0]\n",
    "    }\n",
    "\n",
    "    # Default aggregation functions\n",
    "    _aggregations: dict = {\n",
    "        'tavg': 'mean',\n",
    "        'tmin': 'min',\n",
    "        'tmax': 'max',\n",
    "        'prcp': 'sum',\n",
    "        'snow': 'mean',\n",
    "        'wdir': Core._degree_mean,\n",
    "        'wspd': 'mean',\n",
    "        'wpgt': 'max',\n",
    "        'pres': 'mean',\n",
    "        'tsun': 'sum'\n",
    "    }\n",
    "\n",
    "    def _load(\n",
    "        self,\n",
    "        station: str\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Load file from Meteostat\n",
    "        \"\"\"\n",
    "\n",
    "        # File name\n",
    "        file = 'daily/' + ('full' if self.model else 'obs') + \\\n",
    "            '/' + station + '.csv.gz'\n",
    "\n",
    "        # Get local file path\n",
    "        path = self._get_file_path(self.cache_subdir, file)\n",
    "\n",
    "        # Check if file in cache\n",
    "        if self.max_age > 0 and self._file_in_cache(path):\n",
    "\n",
    "            # Read cached data\n",
    "            df = pd.read_pickle(path)\n",
    "\n",
    "        else:\n",
    "\n",
    "            # Get data from Meteostat\n",
    "            df = self._load_handler(\n",
    "                file,\n",
    "                self._columns,\n",
    "                self._types,\n",
    "                self._parse_dates)\n",
    "\n",
    "            # Validate Series\n",
    "            df = self._validate_series(df, station)\n",
    "\n",
    "            # Save as Pickle\n",
    "            if self.max_age > 0:\n",
    "                df.to_pickle(path)\n",
    "\n",
    "        # Filter time period and append to DataFrame\n",
    "        if self.start and self.end:\n",
    "\n",
    "            # Get time index\n",
    "            time = df.index.get_level_values('time')\n",
    "\n",
    "            # Filter & append\n",
    "            self.data = self.data.append(\n",
    "                df.loc[(time >= self.start) & (time <= self.end)])\n",
    "\n",
    "        else:\n",
    "\n",
    "            # Append\n",
    "            self.data = self.data.append(df)\n",
    "\n",
    "    def _get_data(self) -> None:\n",
    "        \"\"\"\n",
    "        Get all required data\n",
    "        \"\"\"\n",
    "\n",
    "        if len(self.stations) > 0:\n",
    "\n",
    "            # List of datasets\n",
    "            datasets = []\n",
    "\n",
    "            for station in self.stations:\n",
    "                datasets.append((\n",
    "                    str(station),\n",
    "                ))\n",
    "\n",
    "            # Data Processing\n",
    "            self._processing_handler(datasets, self._load, self.max_threads)\n",
    "\n",
    "        else:\n",
    "\n",
    "            # Empty DataFrame\n",
    "            self.data = pd.DataFrame(columns=[*self._types])\n",
    "\n",
    "    def _resolve_point(\n",
    "        self,\n",
    "        method: str,\n",
    "        stations: pd.DataFrame,\n",
    "        alt: int,\n",
    "        adapt_temp: bool\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Project weather station data onto a single point\n",
    "        \"\"\"\n",
    "\n",
    "        if self.stations.size == 0:\n",
    "            return None\n",
    "\n",
    "        if method == 'nearest':\n",
    "\n",
    "            self.data = self.data.groupby(\n",
    "                pd.Grouper(level='time', freq='1D')).agg('first')\n",
    "\n",
    "        else:\n",
    "\n",
    "            # Join score and elevation of involved weather stations\n",
    "            data = self.data.join(\n",
    "                stations[['score', 'elevation']], on='station')\n",
    "\n",
    "            # Adapt temperature-like data based on altitude\n",
    "            if adapt_temp:\n",
    "                data.loc[data['tavg'] != np.NaN, 'tavg'] = data['tavg'] + \\\n",
    "                    ((2 / 3) * ((data['elevation'] - alt) / 100))\n",
    "                data.loc[data['tmin'] != np.NaN, 'tmin'] = data['tmin'] + \\\n",
    "                    ((2 / 3) * ((data['elevation'] - alt) / 100))\n",
    "                data.loc[data['tmax'] != np.NaN, 'tmax'] = data['tmax'] + \\\n",
    "                    ((2 / 3) * ((data['elevation'] - alt) / 100))\n",
    "\n",
    "            # Exclude non-mean data & perform aggregation\n",
    "            excluded = data['wdir']\n",
    "            excluded = excluded.groupby(\n",
    "                pd.Grouper(level='time', freq='1D')).agg('first')\n",
    "\n",
    "            # Aggregate mean data\n",
    "            data = data.groupby(\n",
    "                pd.Grouper(level='time', freq='1D')).apply(self._weighted_average)\n",
    "\n",
    "            # Drop RangeIndex\n",
    "            data.index = data.index.droplevel(1)\n",
    "\n",
    "            # Merge excluded fields\n",
    "            data['wdir'] = excluded\n",
    "\n",
    "            # Drop score and elevation\n",
    "            self.data = data.drop(['score', 'elevation'], axis=1).round(1)\n",
    "\n",
    "        # Set placeholder station ID\n",
    "        self.data['station'] = 'XXXXX'\n",
    "        self.data = self.data.set_index(\n",
    "            ['station', self.data.index.get_level_values('time')])\n",
    "        self.stations = pd.Index(['XXXXX'])\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        loc: Union[pd.DataFrame, Point, list, str],\n",
    "        start: datetime = None,\n",
    "        end: datetime = None,\n",
    "        model: bool = True\n",
    "    ) -> None:\n",
    "\n",
    "        # Set list of weather stations\n",
    "        if isinstance(loc, pd.DataFrame):\n",
    "            self.stations = loc.index\n",
    "        elif isinstance(loc, Point):\n",
    "            stations = loc.get_stations('hourly', start, end)\n",
    "            self.stations = stations.index\n",
    "        else:\n",
    "            if not isinstance(loc, list):\n",
    "                loc = [loc]\n",
    "\n",
    "            self.stations = pd.Index(loc)\n",
    "\n",
    "        # Set start date\n",
    "        self.start = start\n",
    "\n",
    "        # Set end date\n",
    "        self.end = end\n",
    "\n",
    "        # Set model\n",
    "        self.model = model\n",
    "\n",
    "        # Get data for all weather stations\n",
    "        self._get_data()\n",
    "\n",
    "        # Interpolate data\n",
    "        if isinstance(loc, Point):\n",
    "            self._resolve_point(loc.method, stations, loc.alt, loc.adapt_temp)\n",
    "\n",
    "        # Clear cache\n",
    "        if self.max_age > 0:\n",
    "            self.clear_cache()\n",
    "\n",
    "    def normalize(self) -> 'Daily':\n",
    "        \"\"\"\n",
    "        Normalize the DataFrame\n",
    "        \"\"\"\n",
    "\n",
    "        # Create temporal instance\n",
    "        temp = copy(self)\n",
    "\n",
    "        # Create result DataFrame\n",
    "        result = pd.DataFrame(columns=temp._columns[1:])\n",
    "\n",
    "        # Go through list of weather stations\n",
    "        for station in temp.stations:\n",
    "            # Create data frame\n",
    "            df = pd.DataFrame(columns=temp._columns[1:])\n",
    "            # Add time series\n",
    "            df['time'] = pd.date_range(temp.start, temp.end, freq='1D')\n",
    "            # Add station ID\n",
    "            df['station'] = station\n",
    "            # Add columns\n",
    "            for column in temp._columns[1:]:\n",
    "                # Add column to DataFrame\n",
    "                df[column] = NaN\n",
    "\n",
    "            result = pd.concat([result, df], axis=0)\n",
    "\n",
    "        # Set index\n",
    "        result = result.set_index(['station', 'time'])\n",
    "\n",
    "        # Merge data\n",
    "        temp.data = pd.concat([temp.data, result], axis=0).groupby(\n",
    "            ['station', 'time'], as_index=True).first()\n",
    "\n",
    "        # None -> NaN\n",
    "        temp.data = temp.data.fillna(NaN)\n",
    "\n",
    "        # Return class instance\n",
    "        return temp\n",
    "\n",
    "    def interpolate(\n",
    "        self,\n",
    "        limit: int = 3\n",
    "    ) -> 'Daily':\n",
    "        \"\"\"\n",
    "        Interpolate NULL values\n",
    "        \"\"\"\n",
    "\n",
    "        # Create temporal instance\n",
    "        temp = copy(self)\n",
    "\n",
    "        # Apply interpolation\n",
    "        temp.data = temp.data.groupby('station').apply(\n",
    "            lambda group: group.interpolate(\n",
    "                method='linear', limit=limit, limit_direction='both', axis=0))\n",
    "\n",
    "        # Return class instance\n",
    "        return temp\n",
    "\n",
    "    def aggregate(\n",
    "        self,\n",
    "        freq: str = '1D',\n",
    "        spatial: bool = False\n",
    "    ) -> 'Daily':\n",
    "        \"\"\"\n",
    "        Aggregate observations\n",
    "        \"\"\"\n",
    "\n",
    "        # Create temporal instance\n",
    "        temp = copy(self)\n",
    "\n",
    "        # Time aggregation\n",
    "        temp.data = temp.data.groupby(['station', pd.Grouper(\n",
    "            level='time', freq=freq)]).agg(temp._aggregations)\n",
    "\n",
    "        # Spatial aggregation\n",
    "        if spatial:\n",
    "            temp.data = temp.data.groupby(\n",
    "                [pd.Grouper(level='time', freq=freq)]).mean()\n",
    "\n",
    "        # Round\n",
    "        temp.data = temp.data.round(1)\n",
    "\n",
    "        # Return class instance\n",
    "        return temp\n",
    "\n",
    "    def convert(\n",
    "        self,\n",
    "        units: dict\n",
    "    ) -> 'Daily':\n",
    "        \"\"\"\n",
    "        Convert columns to a different unit\n",
    "        \"\"\"\n",
    "\n",
    "        # Create temporal instance\n",
    "        temp = copy(self)\n",
    "\n",
    "        # Change data units\n",
    "        for parameter, unit in units.items():\n",
    "            if parameter in temp._columns:\n",
    "                temp.data[parameter] = temp.data[parameter].apply(unit)\n",
    "\n",
    "        # Return class instance\n",
    "        return temp\n",
    "\n",
    "    def coverage(\n",
    "        self,\n",
    "        parameter: str = None\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Calculate data coverage (overall or by parameter)\n",
    "        \"\"\"\n",
    "\n",
    "        expect = (self.end - self.start).days + 1\n",
    "\n",
    "        if parameter is None:\n",
    "            return len(self.data.index) / expect\n",
    "\n",
    "        return self.data[parameter].count() / expect\n",
    "\n",
    "    def count(self) -> int:\n",
    "        \"\"\"\n",
    "        Return number of rows in DataFrame\n",
    "        \"\"\"\n",
    "\n",
    "        return len(self.data.index)\n",
    "\n",
    "    def fetch(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Fetch DataFrame\n",
    "        \"\"\"\n",
    "\n",
    "        # Copy DataFrame\n",
    "        temp = copy(self.data)\n",
    "\n",
    "        # Remove station index if it's a single station\n",
    "        if len(self.stations) == 1 and 'station' in temp.index.names:\n",
    "            temp = temp.reset_index(level='station', drop=True)\n",
    "\n",
    "        # Return data frame\n",
    "        return temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing data from S3\n",
    "project_dir = (\"https://s3groupaustralia.s3.eu-central-1.amazonaws.com/data/temperature/\")\n",
    "\n",
    "df=pd.read_csv(project_dir + 'newweather.csv')\n",
    "df=df.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all=pd.DataFrame()\n",
    "for row in df:\n",
    "    stations=Stations()\n",
    "    lat=row[1]\n",
    "    lon=row[2]\n",
    "    stations=stations.nearby(lat,lon)\n",
    "    station=stations.fetch(6)\n",
    "    start=datetime(2000,1,1)\n",
    "    end=datetime(2020,12,31)\n",
    "    data = Daily(station, start, end)\n",
    "    data = data.aggregate('M')\n",
    "    data=data.fetch()\n",
    "    data['datetime'] = data.index.get_level_values('time')\n",
    "    data=data.reset_index(drop=True,inplace=False)\n",
    "    a=data.groupby('datetime', as_index=False)['tavg'].mean()\n",
    "    b=data.groupby('datetime', as_index=False)['tmin'].min()\n",
    "    c=data.groupby('datetime', as_index=False)['tmax'].max()\n",
    "    d=data.groupby('datetime', as_index=False)['prcp'].mean()\n",
    "    e=data.groupby('datetime', as_index=False)['wspd'].mean()\n",
    "    final=pd.merge(a,b)\n",
    "    final=pd.merge(final,c)\n",
    "    final=pd.merge(final,d)\n",
    "    final=pd.merge(final,e)\n",
    "    country=row[0]\n",
    "    final.insert(0,'country',country)   \n",
    "    year=row[3]\n",
    "    month=row[4]\n",
    "    heatwave=[]\n",
    "    finallist=final.values.tolist()\n",
    "    heatwave=[]\n",
    "    for j in finallist:\n",
    "        if j[1].year ==  year and j[1].month == month:\n",
    "            heatwave.append('1')\n",
    "        else:\n",
    "            heatwave.append('0')\n",
    "    heatwave=pd.DataFrame(heatwave)\n",
    "    final=pd.concat([final, heatwave], axis=1)\n",
    "    all=all.append(final)\n",
    "all.to_excel('newfinalweather.xlsx')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
